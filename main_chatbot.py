import os
import faiss
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer

class RAGChatbot:
    """
    ë¯¸ë¦¬ êµ¬ì¶•ëœ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰í•˜ëŠ” RAG ì±—ë´‡ í´ë˜ìŠ¤.
    """
    def __init__(self, model_name='upskyy/bge-m3-korean'): # ëª¨ë¸ ì´ë¦„ ì—…ë°ì´íŠ¸
        print("â³ ì±—ë´‡ ì´ˆê¸°í™” ì‹œì‘...")
        
        # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        print(f"  - ì„ë² ë”© ëª¨ë¸({model_name}) ë¡œë“œ ì¤‘...")
        self.model = SentenceTransformer(model_name)
        print("  - ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
        
        # 2. ë²¡í„° ìŠ¤í† ì–´ ë° ë°ì´í„° ë¡œë“œ
        self.artifact_index, self.artifact_df = self._load_vector_store('artifacts')
        self.history_index, self.history_df = self._load_vector_store('history')
        
        print("âœ… ì±—ë´‡ ì´ˆê¸°í™” ì™„ë£Œ. ì§ˆë¬¸ì„ ì…ë ¥í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def _load_vector_store(self, store_name: str):
        """FAISS ì¸ë±ìŠ¤ì™€ ë°ì´í„°í”„ë ˆì„ì„ ë¡œë“œí•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜."""
        index_path = os.path.join('vector_store', f'{store_name}.index')
        df_path = os.path.join('vector_store', f'{store_name}_df.pkl')
        
        if not os.path.exists(index_path) or not os.path.exists(df_path):
            print(f"ğŸš¨ ì¹˜ëª…ì  ì˜¤ë¥˜: '{store_name}'ì˜ ë²¡í„° ìŠ¤í† ì–´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("    vector_store_builder.pyë¥¼ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            exit()
            
        index = faiss.read_index(index_path)
        with open(df_path, 'rb') as f:
            df = pickle.load(f)
            
        print(f"  - '{store_name}' ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ (ì¸ë±ìŠ¤ ë²¡í„° ìˆ˜: {index.ntotal}, ë°ì´í„° í–‰ ìˆ˜: {len(df)})")
        return index, df

    def _route_query(self, query: str):
        """ì§ˆë¬¸ì´ íŠ¹ì • ìœ ë¬¼ì„ ì§€ì¹­í•˜ëŠ”ì§€, ì•„ë‹ˆë©´ ì¼ë°˜ ì—­ì‚¬ ì§ˆë¬¸ì¸ì§€ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
        for name in self.artifact_df['ëª…ì¹­'].tolist():
            if name in query:
                print(f"  ğŸ” ë¼ìš°í„°: '{name}' í‚¤ì›Œë“œ ë°œê²¬ -> 'ìœ ë¬¼ ì •ë³´'ë¡œ ë¶„ë¥˜")
                return "artifact"
        
        print("  ğŸ” ë¼ìš°í„°: íŠ¹ì • ìœ ë¬¼ í‚¤ì›Œë“œ ì—†ìŒ -> 'ì—­ì‚¬ ì •ë³´'ë¡œ ë¶„ë¥˜")
        return "history"

    def _search(self, query: str, search_type: str, k: int = 3):
        """ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        query_embedding = self.model.encode([query])
        
        if search_type == "artifact":
            index = self.artifact_index
            df = self.artifact_df
            text_column = 'rag_document'
            k = 1 
        else: # history
            index = self.history_index
            df = self.history_df
            text_column = 'text_chunk'

        distances, indices = index.search(query_embedding, k)
        
        # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ë©”íƒ€ë°ì´í„°(URL, ì´ë¯¸ì§€ URL ë“±)ë„ í•¨ê»˜ ë°˜í™˜
        retrieved_docs = []
        print("\n  - ê²€ìƒ‰ëœ ì •ë³´ ì¡°ê°(Context):")
        for i, idx in enumerate(indices[0]):
            doc_info = df.iloc[idx].to_dict()
            doc_info['similarity'] = distances[0][i]
            retrieved_docs.append(doc_info)
            print(f"    {i+1}. (ìœ ì‚¬ë„: {doc_info['similarity']:.4f}) {doc_info[text_column][:100]}...")
            
        return retrieved_docs

    def ask(self, query: str):
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ê³  ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        print(f"\n" + "="*50)
        print(f"ğŸ‘¤ ì‚¬ìš©ì ì§ˆë¬¸: {query}")

        query_type = self._route_query(query)
        retrieved_docs = self._search(query, query_type)
        
        # LLMì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ìƒì„±
        context_for_llm = ""
        for doc in retrieved_docs:
            source_file = doc.get('source_file', 'ìœ ë¬¼ DB')
            context_for_llm += f"[ì¶œì²˜: {source_file}]\n{doc.get('rag_document') or doc.get('text_chunk')}\n\n"

        prompt = f"""ë‹¹ì‹ ì€ ë¬´ë ¹ì™•ë¦‰ ì „ë¬¸ AI ë„ìŠ¨íŠ¸ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë°˜ë“œì‹œ ì•„ë˜ [ì°¸ê³  ìë£Œ]ì—ë§Œ ê·¼ê±°í•˜ì—¬ ì‚¬ìš©ìì˜ [ì§ˆë¬¸]ì— ëŒ€í•´ ë‹µë³€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ë¡œ ì§€ì–´ë‚´ì§€ ë§ê³ , "ìë£Œì— ì—†ëŠ” ë‚´ìš©ì´ë¼ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ì†”ì§í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

---
[ì°¸ê³  ìë£Œ]
{context_for_llm.strip()}
---

[ì§ˆë¬¸]
{query}

[ë‹µë³€]
"""
        
        print("\n" + "="*50)
        print("ğŸ¤– ìµœì¢… ìƒì„± í”„ë¡¬í”„íŠ¸ (ì´ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ LLMì´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤):")
        print(prompt)
        
        # ì‹¤ì œ LLM API í˜¸ì¶œ ëŒ€ì‹ , ì—¬ê¸°ì„œëŠ” ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ì™€ ì°¸ì¡° ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        # ì´ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ì›¹ ì•±ì—ì„œëŠ” ë‹µë³€ê³¼ í•¨ê»˜ ì´ë¯¸ì§€, URL ë“±ì„ í‘œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        final_result = {
            "prompt": prompt,
            "retrieved_docs": retrieved_docs
        }
        
        print("âœ… RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ.")
        return final_result

# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
if __name__ == '__main__':
    chatbot = RAGChatbot()
    
    # --- í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ---
    result1 = chatbot.ask("ì™•ë¹„ì˜ ì€íŒ”ì°ŒëŠ” ì–´ë–¤ ëª¨ì–‘ì´ì•¼?")
    # ë‹µë³€ê³¼ í•¨ê»˜ ì´ë¯¸ì§€ URLì„ í‘œì‹œí•˜ëŠ” ë°©ë²• (ì›¹ ì•±ì—ì„œì˜ í™œìš© ì˜ˆì‹œ)
    if result1['retrieved_docs'] and 'image_url' in result1['retrieved_docs'][0]:
        print(f"\nğŸ–¼ï¸ ê´€ë ¨ ì´ë¯¸ì§€ URL: {result1['retrieved_docs'][0]['image_url']}")

    result2 = chatbot.ask("ë¬´ë ¹ì™•ë¦‰ ì§€ì„ì˜ ê¸€ì”¨ì²´ëŠ” ì–´ë–¤ íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆë‚˜ìš”?")