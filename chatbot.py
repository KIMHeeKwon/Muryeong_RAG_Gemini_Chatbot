# chatbot.py
import os
import faiss
import pickle
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
import config
import numpy as np
import json
from dotenv import load_dotenv

class RAGChatbot:
    # ... (__new__, __init__, _load_vector_store, _semantic_route_query, _search í•¨ìˆ˜ëŠ” ì´ì „ê³¼ ë™ì¼í•©ë‹ˆë‹¤) ...
    _instance = None
    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(RAGChatbot, cls).__new__(cls, *args, **kwargs)
        return cls._instance

    def __init__(self):
        if hasattr(self, '_initialized'): return
        load_dotenv()
        print("â³ ì±—ë´‡ ì´ˆê¸°í™” ì‹œì‘...")
        self.model = SentenceTransformer(config.EMBEDDING_MODEL)
        self.artifact_index, self.artifact_df = self._load_vector_store('artifacts', config.ARTIFACT_INDEX_PATH, config.ARTIFACT_DF_PATH)
        self.history_index, self.history_df = self._load_vector_store('history', config.HISTORY_INDEX_PATH, config.HISTORY_DF_PATH)
        try:
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key: raise ValueError("API í‚¤ê°€ .envì— ì—†ìŠµë‹ˆë‹¤.")
            genai.configure(api_key=api_key)
            self.llm_model = genai.GenerativeModel(config.LLM_MODEL)
            print("  - Google Gemini ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
        except Exception as e:
            self.llm_model = None
            print(f"  - ğŸš¨ ê²½ê³ : Gemini ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - {e}")
        self._initialized = True
        print("âœ… ì±—ë´‡ ì´ˆê¸°í™” ì™„ë£Œ.")

    def _load_vector_store(self, store_name, index_path, df_path):
        if not os.path.exists(index_path) or not os.path.exists(df_path):
            raise FileNotFoundError(f"'{store_name}'ì˜ ë²¡í„° ìŠ¤í† ì–´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"  - '{store_name}' ë²¡í„° ìŠ¤í† ì–´ ë¡œë”©...")
        index = faiss.read_index(index_path)
        with open(df_path, 'rb') as f:
            df = pickle.load(f)
        return index, df

    def _semantic_route_query(self, query: str, chat_history: list):
        if not chat_history:
            history_str = "ì—†ìŒ"
        else:
            history_str = "\n".join([f"{msg['role']}: {msg['parts'][0]}" for msg in chat_history])

        routing_prompt = f"""ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ë¶„ì„í•˜ëŠ” ë¼ìš°íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ [ì´ì „ ëŒ€í™” ë‚´ìš©]ê³¼ [ì‚¬ìš©ì ì§ˆë¬¸]ì„ ë³´ê³ , ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ì•„ë˜ [ì§ˆë¬¸ ìœ í˜•] ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì§ˆë¬¸ ìœ í˜•]
- "ìœ ë¬¼_ìƒì„¸ì •ë³´": íŠ¹ì • ìœ ë¬¼ í•˜ë‚˜ì— ëŒ€í•œ ìƒì„¸ ì •ë³´(ëª¨ì–‘, ì¬ì§ˆ, ì¶œí†  ìœ„ì¹˜ ë“±)ë¥¼ ë¬»ëŠ” ì§ˆë¬¸.
- "ì—­ì‚¬_ë°°ê²½": íŠ¹ì • ì‹œëŒ€, ì‚¬ê±´, ê¸°ìˆ , ë¬¸í™” ë“± í¬ê´„ì ì¸ ì—­ì‚¬ì  ë°°ê²½ì´ë‚˜ ì§€ì‹ì„ ë¬»ëŠ” ì§ˆë¬¸.
- "ìœ ë¬¼_ë¹„êµ": ë‘ ê°œ ì´ìƒì˜ ìœ ë¬¼ì„ ë¹„êµí•´ë‹¬ë¼ëŠ” ì§ˆë¬¸.
- "ë‹¨ìˆœ_ëŒ€í™”": ì •ë³´ ê²€ìƒ‰ì´ í•„ìš” ì—†ëŠ” ì¼ë°˜ì ì¸ ëŒ€í™” (ì¸ì‚¬, ê°ì‚¬ ë“±).

[ì´ì „ ëŒ€í™” ë‚´ìš©]
{history_str}

[ì‚¬ìš©ì ì§ˆë¬¸]
"{query}"

[ë¶„ì„ ê²°ê³¼ (JSON í˜•ì‹)]
"""
        try:
            response = self.llm_model.generate_content(
                f'{routing_prompt}\n{{\n  "classification": "ì—¬ê¸°ì— ë¶„ë¥˜ ê²°ê³¼ ì…ë ¥",\n  "reason": "ë¶„ë¥˜ ì´ìœ  ìš”ì•½"\n}}'
            )
            json_text = response.text.strip().replace('```json', '').replace('```', '').strip()
            route_result = json.loads(json_text)
            classification = route_result.get("classification", "ì—­ì‚¬_ë°°ê²½")
            print(f"  ğŸ§  ì‹œë§¨í‹± ë¼ìš°í„°: '{classification}'ìœ¼ë¡œ ë¶„ë¥˜. (ì´ìœ : {route_result.get('reason')})")
            return classification
        except Exception as e:
            print(f"ğŸš¨ ë¼ìš°íŒ… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. 'ì—­ì‚¬_ë°°ê²½'ìœ¼ë¡œ ê¸°ë³¸ ì„¤ì •í•©ë‹ˆë‹¤.")
            return "ì—­ì‚¬_ë°°ê²½"

    def _search(self, query: str, route: str, k: int = 3):
        query_embedding = self.model.encode([query])
        if route == "ìœ ë¬¼_ìƒì„¸ì •ë³´":
            distances, indices = self.artifact_index.search(query_embedding, 1)
            return [self.artifact_df.iloc[idx].to_dict() for idx in indices[0]]
        elif route == "ìœ ë¬¼_ë¹„êµ":
            distances, indices = self.artifact_index.search(query_embedding, k)
            return [self.artifact_df.iloc[idx].to_dict() for idx in indices[0]]
        elif route == "ì—­ì‚¬_ë°°ê²½":
            distances, indices = self.history_index.search(query_embedding, k)
            return [self.history_df.iloc[idx].to_dict() for idx in indices[0]]
        else:
            return []

    def ask(self, query: str, chat_history: list = []):
        if not self.llm_model:
            return {"error": "Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        route = self._semantic_route_query(query, chat_history)
        
        if route == "ë‹¨ìˆœ_ëŒ€í™”":
            try:
                response = self.llm_model.generate_content(f"ì‚¬ìš©ìê°€ ë‹¤ìŒê³¼ ê°™ì´ ë§í–ˆìŠµë‹ˆë‹¤: '{query}'. ê°„ë‹¨í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.")
                return {"answer": response.text, "metadata": []}
            except Exception as e:
                return {"error": f"Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}

        retrieved_docs = self._search(query, route)
        
        context_for_llm = ""
        for doc in retrieved_docs:
            source = doc.get('source_file', 'ìœ ë¬¼ DB')
            context_for_llm += f"### ì°¸ê³  ìë£Œ (ì¶œì²˜: {source}) ###\n"
            if 'ëª…ì¹­' in doc and not doc.get('source_file'):
                context_for_llm += f"ìœ ë¬¼ëª…: {doc.get('ëª…ì¹­', 'N/A')}\n"
                context_for_llm += f"ì†Œì¥í’ˆë²ˆí˜¸: {doc.get('ì†Œì¥í’ˆë²ˆí˜¸', 'N/A')}\n"
            context_for_llm += f"ë‚´ìš©: {doc.get('rag_document') or doc.get('text_chunk')}\n"
            if 'MUCH_URL' in doc and doc['MUCH_URL']:
                context_for_llm += f"ê´€ë ¨ ë§í¬: {doc['MUCH_URL']}\n"
            context_for_llm += "\n"

        formatted_history = "\n".join([f"{msg['role']}: {msg['parts'][0]}" for msg in chat_history])
        
        # (â­ í•µì‹¬ ìˆ˜ì •) í”„ë¡¬í”„íŠ¸ì˜ ì§€ì‹œì‚¬í•­ì„ ë” ëª…í™•í•˜ê²Œ ë³€ê²½
        prompt = f"""ë‹¹ì‹ ì€ êµ­ë¦½ê³µì£¼ë°•ë¬¼ê´€ì˜ ì „ë¬¸ AI ë„ìŠ¨íŠ¸ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë°˜ë“œì‹œ ì•„ë˜ [ì´ì „ ëŒ€í™” ë‚´ìš©]ê³¼ [ì°¸ê³  ìë£Œ]ì—ë§Œ ê·¼ê±°í•˜ì—¬ ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ [ì§ˆë¬¸]ì— ëŒ€í•´ ë‹µë³€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…ì²´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

[ì¤‘ìš” ê·œì¹™]
- ì¼ë°˜ì ì¸ ìœ ë¬¼ ì„¤ëª…ì—ëŠ” ì ˆëŒ€ë¡œ 'ì†Œì¥í’ˆë²ˆí˜¸'ì™€ ê°™ì€ ë‚´ë¶€ ê´€ë¦¬ ë²ˆí˜¸ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
- ë‹¨, ì‚¬ìš©ìê°€ 'ìœ ë¬¼ë²ˆí˜¸'ë‚˜ 'ì†Œì¥í’ˆë²ˆí˜¸'ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë¬¼ì–´ë³´ëŠ” ê²½ìš°ì—ë§Œ, ì°¸ê³  ìë£Œì— ìˆëŠ” 'ì†Œì¥í’ˆë²ˆí˜¸' ê°’ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.

---
[ì´ì „ ëŒ€í™” ë‚´ìš©]
{formatted_history}
---
[ì°¸ê³  ìë£Œ]
{context_for_llm.strip()}
---

[ì§ˆë¬¸]
{query}

[ë‹µë³€]
"""
        try:
            response = self.llm_model.generate_content(prompt)
            return {
                "answer": response.text,
                "metadata": retrieved_docs
            }
        except Exception as e:
            return {"error": f"Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}

chatbot_instance = RAGChatbot()
