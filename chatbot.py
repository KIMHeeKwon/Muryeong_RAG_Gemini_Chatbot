# chatbot.py
import os
import faiss
import pickle
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
import config
import numpy as np
import json
from dotenv import load_dotenv

class RAGChatbot:
    _instance = None
    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(RAGChatbot, cls).__new__(cls, *args, **kwargs)
        return cls._instance

    def __init__(self):
        if hasattr(self, '_initialized'): return
        load_dotenv()
        print("â³ ì±—ë´‡ ì´ˆê¸°í™” ì‹œì‘...")
        self.model = SentenceTransformer(config.EMBEDDING_MODEL)
        self.artifact_index, self.artifact_df = self._load_vector_store('artifacts', config.ARTIFACT_INDEX_PATH, config.ARTIFACT_DF_PATH)
        self.history_index, self.history_df = self._load_vector_store('history', config.HISTORY_INDEX_PATH, config.HISTORY_DF_PATH)
        try:
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key: raise ValueError("API í‚¤ê°€ .envì— ì—†ìŠµë‹ˆë‹¤.")
            genai.configure(api_key=api_key)
            self.llm_model = genai.GenerativeModel(config.LLM_MODEL)
            print("  - Google Gemini ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
        except Exception as e:
            self.llm_model = None
            print(f"  - ğŸš¨ ê²½ê³ : Gemini ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - {e}")
        self._initialized = True
        print("âœ… ì±—ë´‡ ì´ˆê¸°í™” ì™„ë£Œ.")

    def _load_vector_store(self, store_name, index_path, df_path):
        if not os.path.exists(index_path) or not os.path.exists(df_path):
            raise FileNotFoundError(f"'{store_name}'ì˜ ë²¡í„° ìŠ¤í† ì–´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"  - '{store_name}' ë²¡í„° ìŠ¤í† ì–´ ë¡œë”©...")
        index = faiss.read_index(index_path)
        with open(df_path, 'rb') as f:
            df = pickle.load(f)
        return index, df

    # (â­ í•µì‹¬ ì¶”ê°€ 1) ì§ˆë¬¸ ì¬êµ¬ì„± í•¨ìˆ˜
    def _rewrite_query_with_history(self, query: str, chat_history: list):        
        """ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ ì§ˆë¬¸ì„ ì™„ì „í•œ ê²€ìƒ‰ìš© ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤."""
        if not chat_history:
            return query  # ëŒ€í™” ê¸°ë¡ì´ ì—†ìœ¼ë©´ ì›ë³¸ ì§ˆë¬¸ ì‚¬ìš©

        # âœ… content/parts ì–´ëŠ ìª½ì´ ì™€ë„ ì•ˆì „í•˜ê²Œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
        def _msg_text(msg: dict) -> str:
            if "content" in msg:
                return str(msg["content"])
            parts = msg.get("parts")
            if isinstance(parts, list):
                return "".join(str(p) for p in parts)
            return str(parts) if parts is not None else ""

        lines = []
        for m in chat_history:
            role = m.get("role")
            speaker = "ì‚¬ìš©ì" if role == "user" else "ì§„ë¬˜"
            lines.append(f"{speaker}: {_msg_text(m)}")
        formatted_history = "\n".join(lines)

        rewrite_prompt = f"""ì´ì „ ëŒ€í™” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
---
{formatted_history}
---
ìœ„ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬, ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ì¸ "{query}"ë¥¼, ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ëª¨ë¥´ëŠ” ì‚¬ëŒë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ì™„ì „í•˜ê³  ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.
ì¬ì‘ì„±ëœ ì§ˆë¬¸ì€ ë‹¤ë¥¸ ì–´ë–¤ ì„¤ëª…ë„ ì—†ì´, ì˜¤ì§ ì§ˆë¬¸ ë¬¸ì¥ í•˜ë‚˜ë§Œ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
"""
        try:
            response = self.llm_model.generate_content(rewrite_prompt)
            rewritten = (response.text or "").strip().replace('"', "")
            return rewritten or query
        except Exception as e:
            print(f"ğŸš¨ ì§ˆë¬¸ ì¬êµ¬ì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return query

    
    def _semantic_route_query(self, query: str): # (â­ ìˆ˜ì •) ì´ì œ ëŒ€í™” ê¸°ë¡ì´ í•„ìš” ì—†ìŒ
        routing_prompt = f"""ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ë¶„ì„í•˜ëŠ” ë¼ìš°íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
[ì‚¬ìš©ì ì§ˆë¬¸]ì„ ë³´ê³ , ì˜ë„ë¥¼ ì•„ë˜ [ì§ˆë¬¸ ìœ í˜•] ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì§ˆë¬¸ ìœ í˜•]
- "ìœ ë¬¼_ìƒì„¸ì •ë³´": íŠ¹ì • ìœ ë¬¼ í•˜ë‚˜ì— ëŒ€í•œ ìƒì„¸ ì •ë³´(ëª¨ì–‘, ì¬ì§ˆ, ì¶œí†  ìœ„ì¹˜ ë“±)ë¥¼ ë¬»ëŠ” ì§ˆë¬¸.
- "ì—­ì‚¬_ë°°ê²½": íŠ¹ì • ì‹œëŒ€, ì‚¬ê±´, ê¸°ìˆ , ë¬¸í™” ë“± í¬ê´„ì ì¸ ì—­ì‚¬ì  ë°°ê²½ì´ë‚˜ ì§€ì‹ì„ ë¬»ëŠ” ì§ˆë¬¸.
- "ìœ ë¬¼_ë¹„êµ": ë‘ ê°œ ì´ìƒì˜ ìœ ë¬¼ì„ ë¹„êµí•´ë‹¬ë¼ëŠ” ì§ˆë¬¸.
- "ë‹¨ìˆœ_ëŒ€í™”": ì •ë³´ ê²€ìƒ‰ì´ í•„ìš” ì—†ëŠ” ì¼ë°˜ì ì¸ ëŒ€í™” (ì¸ì‚¬, ê°ì‚¬ ë“±).

[ì‚¬ìš©ì ì§ˆë¬¸]
"{query}"

[ë¶„ì„ ê²°ê³¼ (JSON í˜•ì‹)]
"""
        try:
            response = self.llm_model.generate_content(f'{routing_prompt}\n{{\n  "classification": "...",\n  "reason": "..."\n}}')
            json_text = response.text.strip().replace('```json', '').replace('```', '').strip()
            route_result = json.loads(json_text)
            classification = route_result.get("classification", "ì—­ì‚¬_ë°°ê²½")
            print(f"  ğŸ§  ì‹œë§¨í‹± ë¼ìš°í„°: '{classification}'ìœ¼ë¡œ ë¶„ë¥˜. (ì´ìœ : {route_result.get('reason')})")
            return classification
        except Exception as e:
            print(f"ğŸš¨ ë¼ìš°íŒ… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. 'ì—­ì‚¬_ë°°ê²½'ìœ¼ë¡œ ê¸°ë³¸ ì„¤ì •í•©ë‹ˆë‹¤.")
            return "ì—­ì‚¬_ë°°ê²½"

    def _search(self, query: str, route: str, k: int = 3):
        query_embedding = self.model.encode([query])
        if route == "ìœ ë¬¼_ìƒì„¸ì •ë³´":
            distances, indices = self.artifact_index.search(query_embedding, 1)
            return [self.artifact_df.iloc[idx].to_dict() for idx in indices[0]]
        elif route == "ìœ ë¬¼_ë¹„êµ":
            distances, indices = self.artifact_index.search(query_embedding, k)
            return [self.artifact_df.iloc[idx].to_dict() for idx in indices[0]]
        elif route == "ì—­ì‚¬_ë°°ê²½":
            distances, indices = self.history_index.search(query_embedding, k)
            return [self.history_df.iloc[idx].to_dict() for idx in indices[0]]
        else:
            return []

    def ask(self, query: str, chat_history: list | None = None):
        if chat_history is None:
            chat_history = []
        if not self.llm_model: return {"error": "Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        # (â­ í•µì‹¬ ì¶”ê°€ 2) ë¼ìš°íŒ… ì „ì— ì§ˆë¬¸ ì¬êµ¬ì„± ì‹¤í–‰
        rewritten_query = self._rewrite_query_with_history(query, chat_history)
        
        route = self._semantic_route_query(rewritten_query)
        
        if route == "ë‹¨ìˆœ_ëŒ€í™”":
            try:
                response = self.llm_model.generate_content(f"ì‚¬ìš©ìê°€ ë‹¤ìŒê³¼ ê°™ì´ ë§í–ˆìŠµë‹ˆë‹¤: '{query}'. ê°„ë‹¨í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.")
                return {"answer": response.text, "metadata": []}
            except Exception as e:
                return {"error": f"Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}

        # ì¬êµ¬ì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰
        retrieved_docs = self._search(rewritten_query, route)
        
        context_for_llm = ""
        for doc in retrieved_docs:
            source = doc.get('source_file', 'ìœ ë¬¼ DB: ' + doc.get('ëª…ì¹­', ''))
            context_for_llm += f"### ì°¸ê³  ìë£Œ (ì¶œì²˜: {source}) ###\n"
            context_for_llm += f"ë‚´ìš©: {doc.get('rag_document') or doc.get('text_chunk')}\n"
            if 'MUCH_URL' in doc and doc['MUCH_URL']: context_for_llm += f"ê´€ë ¨ ë§í¬: {doc['MUCH_URL']}\n"
            if 'id' in doc and doc['id']: context_for_llm += f"ìœ ë¬¼ ID: {doc['id']}\n"
            context_for_llm += "\n"
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ì—ëŠ” ì „ì²´ ëŒ€í™” ê¸°ë¡ê³¼ ì›ë³¸ ì§ˆë¬¸ì„ ì‚¬ìš©
        formatted_history = "\n".join([f"ì‚¬ìš©ì: {msg['content']}" if msg['role'] == 'user' else f"ì§„ë¬˜: {msg['content']}" for msg in chat_history])
        prompt = f"""ë‹¹ì‹ ì€ êµ­ë¦½ê³µì£¼ë°•ë¬¼ê´€ì˜ ì „ë¬¸ AI ë„ìŠ¨íŠ¸ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë°˜ë“œì‹œ ì•„ë˜ [ì´ì „ ëŒ€í™” ë‚´ìš©]ê³¼ [ì°¸ê³  ìë£Œ]ì—ë§Œ ê·¼ê±°í•˜ì—¬ ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ [ì§ˆë¬¸]ì— ëŒ€í•´ ë‹µë³€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…ì²´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ë¡œ ì§€ì–´ë‚´ì§€ ë§ê³ , "ìë£Œì— ì—†ëŠ” ë‚´ìš©ì´ë¼ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ì†”ì§í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

---
[ì´ì „ ëŒ€í™” ë‚´ìš©]
{formatted_history}
---
[ì°¸ê³  ìë£Œ]
{context_for_llm.strip()}
---

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

[ë‹µë³€]
"""
        try:
            response = self.llm_model.generate_content(prompt)
            return {"answer": response.text, "metadata": retrieved_docs}
        except Exception as e:
            return {"error": f"Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}

chatbot_instance = RAGChatbot()
